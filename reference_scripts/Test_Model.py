# 27 Jul 2023
# Generate class-specific performance metrics for prospective PNW-Cnet v5 models.
# Will compare class scores generated by the model to n-hot labels on a subset
# of the training dataset. Need to pull this subset (call it 5% of the existing
# training set, leave validation set alone) and put it in its own CSV file.
# Script will take a model file as an argument, use said model to make predictions
# on the test set, compare the scores to the labels and summarize performance
# (precision and recall) by class.

import os
import sys
import time

import numpy as np
import pandas as pd
import tensorflow as tf

from pathlib import Path
from tensorflow.keras.models import load_model

# Join the predictions data frame to the labels data frame. Compare class scores
# to binary labels and generate available, apparent, TP, FP, TN, FN, precision,
# recall, F1, and accuracy at the given threshold.
def generatePerformanceMetrics(test_df_path, pred_df_path, class_names, threshold = 0.95):
    test_df = pd.read_csv(test_df_path)
    pred_df = pd.read_csv(pred_df_path)

    test_df = test_df.drop(columns = ["Folder", "Set", "Path"])
    pred_df = pred_df.rename(mapper = lambda x: x if x in ["Filename", "Folder"] else f"p{x}", axis = "columns")

    joined_df = test_df.merge(pred_df, on = "Filename", how = "inner")

    metric_col_names = ["Class", "Threshold", "Available", "Apparent", "TP", "FP", "TN", "FN", "Precision", "Recall", "F1", "Accuracy"]
    metrics = []
    for i in class_names:
        pred_col = f"p{i}"
        n_avail = sum(joined_df[i])
        n_apparent = len(joined_df[joined_df[f"p{i}"] >= threshold])
        n_TP = len(joined_df[(joined_df[f"p{i}"] >= threshold) & (joined_df[i] == 1)])
        n_FP = n_apparent - n_TP
        n_TN = len(joined_df[(joined_df[f"p{i}"] < threshold) & (joined_df[i] == 0)])
        n_FN = n_avail - n_TP
        try:
            precision = n_TP / n_apparent
            recall = n_TP / n_avail
            f1 = (2 * precision * recall) / (precision + recall)
            accuracy = sum([n_TP, n_TN]) / sum([n_TP, n_FP, n_TN, n_FN])
        except:
            precision = np.nan
            recall = np.nan
            f1 = np.nan
            accuracy = np.nan
        metrics.append([i, threshold, n_avail, n_apparent, n_TP, n_FP, n_TN, n_FN, precision, recall, f1, accuracy])

    metric_df = pd.DataFrame(data = metrics, columns = metric_col_names)
    metric_df = metric_df.round({"Precision":4, "Recall":4, "F1":4, "Accuracy":4})

    return metric_df

################################################################################

def main():
    model_path = sys.argv[1]
    
    top_dir = "./data"
    split_csv = Path(top_dir, "Train_Val_Split.csv")
    test_csv = Path(top_dir, "Test_Set.csv")
    
    batch_size = 128

    test_set = pd.read_csv(test_csv)

    if not os.path.exists(model_path):
        print("Could not find saved model at the path specified.")
        exit()
    else:
        model = load_model(model_path)

    target_classes ="ACCO1,ACGE1,ACGE2,ACST1,AEAC1,AEAC2,Airplane,ANCA1,ASOT1,BOUM1,BRCA1,BRMA1,BRMA2,BUJA1,BUJA2,Bullfrog,BUVI1,BUVI2,CACA1,CAGU1,CAGU2,CAGU3,CALA1,CALU1,CAPU1,CAUS1,CAUS2,CCOO1,CCOO2,CECA1,Chainsaw,CHFA1,Chicken,CHMI1,CHMI2,COAU1,COAU2,COBR1,COCO1,COSO1,Cow,Creek,Cricket,CYST1,CYST2,DEFU1,DEFU2,Dog,DRPU1,Drum,EMDI1,EMOB1,FACO1,FASP1,Fly,Frog,GADE1,GLGN1,Growler,Gunshot,HALE1,HAPU1,HEVE1,Highway,Horn,Human,HYPI1,IXNA1,IXNA2,JUHY1,LEAL1,LECE1,LEVI1,LEVI2,LOCU1,MEFO1,MEGA1,MEKE1,MEKE2,MEKE3,MYTO1,NUCO1,OCPR1,ODOC1,ORPI1,ORPI2,PAFA1,PAFA2,PAHA1,PECA1,PHME1,PHNU1,PILU1,PILU2,PIMA1,PIMA2,POEC1,POEC2,PSFL1,Rain,Raptor,SICU1,SITT1,SITT2,SPHY1,SPHY2,SPPA1,SPPI1,SPTH1,STDE1,STNE1,STNE2,STOC_4Note,STOC_Series,Strix_Bark,Strix_Whistle,STVA_8Note,STVA_Insp,STVA_Series,Survey_Tone,TADO1,TADO2,TAMI1,Thunder,TRAE1,Train,Tree,TUMI1,TUMI2,URAM1,VIHU1,Wildcat,Yarder,ZEMA1,ZOLE1".split(',')

    starttime = time.time()
    print("Making predictions on {0} images starting at {1}...".format(len(test_set), time.strftime("%H:%M:%S")))

    predict_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)

    test_data = predict_datagen.flow_from_dataframe(
        dataframe = test_set, 
        directory = None,
        x_col = "Path",
        shuffle = False,
        target_size = (257,1000),
        batch_size = batch_size,
        color_mode = "grayscale",
        class_mode = None)
    
    n_steps = int(len(test_set) / batch_size) + 1
    
    preds_raw = model.predict(
        x = test_data,
        steps = n_steps,
        verbose = 1)
    
    paths = test_set["Path"]
    folders = [os.path.dirname(x) for x in paths]
    filenames = [os.path.basename(y) for y in paths]
    
    preds = pd.DataFrame(data = preds_raw, columns = target_classes).round(decimals = 4)
    preds.insert(loc = 0, column = "Folder", value = folders)
    preds.insert(loc = 0, column = "Filename", value = filenames)
    
    pred_csv = os.path.join(top_dir, "Predictions_{0}.csv".format(os.path.basename(model_path).replace(".h5", "")))
    
    preds.to_csv(pred_csv, index = False)
    
    metric_df = generatePerformanceMetrics(test_csv, pred_csv, target_classes, 0.25)
    for thresh in [0.50, 0.75, 0.95]:
        new_df = generatePerformanceMetrics(test_csv, pred_csv, target_classes, thresh)
        metric_df = pd.concat(objs = [metric_df, new_df])
    
    metric_csv = pred_csv.replace("Predictions", "Test_Metrics")
    metric_df.to_csv(metric_csv, index = False)

if __name__ == "__main__":
    main()