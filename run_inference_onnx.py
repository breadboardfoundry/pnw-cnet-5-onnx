"""
Run inference on spectrogram images using the PNW-Cnet-5 ONNX model.

This script loads spectrogram images generated by wav_to_spectrogram.py
and runs them through the ONNX-converted classifier to predict species.
"""

import argparse
import os
from pathlib import Path

import numpy as np
import onnxruntime as ort
from PIL import Image


# Class labels for the 135 bioacoustic categories (from Train_Model.py)
CLASSES = [
    "ACCO1", "ACGE1", "ACGE2", "ACST1", "AEAC1", "AEAC2", "Airplane", "ANCA1",
    "ASOT1", "BOUM1", "BRCA1", "BRMA1", "BRMA2", "BUJA1", "BUJA2", "Bullfrog",
    "BUVI1", "BUVI2", "CACA1", "CAGU1", "CAGU2", "CAGU3", "CALA1", "CALU1",
    "CAPU1", "CAUS1", "CAUS2", "CCOO1", "CCOO2", "CECA1", "Chainsaw", "CHFA1",
    "Chicken", "CHMI1", "CHMI2", "COAU1", "COAU2", "COBR1", "COCO1", "COSO1",
    "Cow", "Creek", "Cricket", "CYST1", "CYST2", "DEFU1", "DEFU2", "Dog",
    "DRPU1", "Drum", "EMDI1", "EMOB1", "FACO1", "FASP1", "Fly", "Frog",
    "GADE1", "GLGN1", "Growler", "Gunshot", "HALE1", "HAPU1", "HEVE1",
    "Highway", "Horn", "Human", "HYPI1", "IXNA1", "IXNA2", "JUHY1", "LEAL1",
    "LECE1", "LEVI1", "LEVI2", "LOCU1", "MEFO1", "MEGA1", "MEKE1", "MEKE2",
    "MEKE3", "MYTO1", "NUCO1", "OCPR1", "ODOC1", "ORPI1", "ORPI2", "PAFA1",
    "PAFA2", "PAHA1", "PECA1", "PHME1", "PHNU1", "PILU1", "PILU2", "PIMA1",
    "PIMA2", "POEC1", "POEC2", "PSFL1", "Rain", "Raptor", "SICU1", "SITT1",
    "SITT2", "SPHY1", "SPHY2", "SPPA1", "SPPI1", "SPTH1", "STDE1", "STNE1",
    "STNE2", "STOC_4Note", "STOC_Series", "Strix_Bark", "Strix_Whistle",
    "STVA_8Note", "STVA_Insp", "STVA_Series", "Survey_Tone", "TADO1", "TADO2",
    "TAMI1", "Thunder", "TRAE1", "Train", "Tree", "TUMI1", "TUMI2", "URAM1",
    "VIHU1", "Wildcat", "Yarder", "ZEMA1", "ZOLE1",
]

DEFAULT_MODEL_PATH = "model/Final_Model.onnx"


def load_image(image_path: str) -> np.ndarray:
    """
    Load and preprocess a spectrogram image.

    Args:
        image_path: Path to the PNG image

    Returns:
        Preprocessed image array of shape (257, 1000, 1)
    """
    # Load image as grayscale
    img = Image.open(image_path).convert("L")

    # Resize to expected dimensions
    img = img.resize((1000, 257), Image.Resampling.BILINEAR)

    # Convert to numpy array and normalize to [0, 1]
    img_array = np.array(img, dtype=np.float32) / 255.0

    # Add channel dimension
    img_array = img_array[:, :, np.newaxis]

    return img_array


def load_images_batch(image_paths: list[str]) -> np.ndarray:
    """
    Load multiple images into a batch.

    Args:
        image_paths: List of image file paths

    Returns:
        Batch of images with shape (N, 257, 1000, 1)
    """
    images = [load_image(p) for p in image_paths]
    return np.stack(images, axis=0)


def run_inference(
    input_dir: str,
    model_path: str,
    threshold: float = 0.5,
    top_k: int = 5,
    batch_size: int = 32,
) -> dict:
    """
    Run inference on all spectrogram images in a directory.

    Args:
        input_dir: Directory containing spectrogram PNG files
        model_path: Path to the ONNX model
        threshold: Confidence threshold for predictions
        top_k: Number of top predictions to show per image
        batch_size: Batch size for inference

    Returns:
        Dictionary mapping filenames to predictions
    """
    # Load ONNX model
    print(f"Loading ONNX model from: {model_path}")
    session = ort.InferenceSession(
        model_path,
        providers=["CoreMLExecutionProvider", "CPUExecutionProvider"],
    )

    # Get input/output names
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    print(f"  Input name: {input_name}")
    print(f"  Input shape: {session.get_inputs()[0].shape}")
    print(f"  Output name: {output_name}")

    # Find all PNG files
    input_path = Path(input_dir)
    png_files = sorted(input_path.glob("*.png"))

    print(f"Loading spectrograms from: {input_dir}")
    print(f"Found {len(png_files)} images")

    # Process in batches
    print("Running inference...")
    results = {}
    total_batches = (len(png_files) + batch_size - 1) // batch_size

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(png_files))
        batch_files = png_files[start_idx:end_idx]

        # Load batch
        batch_images = load_images_batch([str(f) for f in batch_files])

        # Run inference
        predictions = session.run([output_name], {input_name: batch_images})[0]

        # Process results for each image in batch
        for i, filepath in enumerate(batch_files):
            pred = predictions[i]
            basename = filepath.name

            # Get top-k predictions
            top_indices = np.argsort(pred)[::-1][:top_k]
            top_preds = [
                (CLASSES[idx], float(pred[idx]))
                for idx in top_indices
            ]

            # Get predictions above threshold
            above_threshold = [
                (CLASSES[idx], float(pred[idx]))
                for idx in range(len(pred))
                if pred[idx] >= threshold
            ]
            above_threshold.sort(key=lambda x: x[1], reverse=True)

            results[basename] = {
                "top_k": top_preds,
                "above_threshold": above_threshold,
                "raw_predictions": pred,
            }

        print(f"  Processed batch {batch_idx + 1}/{total_batches}")

    return results


def print_results(results: dict, threshold: float, top_k: int) -> None:
    """Print inference results in a readable format."""
    print("\n" + "=" * 60)
    print("INFERENCE RESULTS")
    print("=" * 60)

    for filename, data in sorted(results.items()):
        print(f"\n{filename}")
        print("-" * 40)

        if data["above_threshold"]:
            print(f"  Detections (threshold >= {threshold}):")
            for label, conf in data["above_threshold"]:
                print(f"    {label}: {conf:.4f}")
        else:
            print(f"  No detections above threshold {threshold}")

        print(f"  Top {top_k} predictions:")
        for label, conf in data["top_k"]:
            print(f"    {label}: {conf:.4f}")


def save_results_csv(results: dict, output_path: str) -> None:
    """Save results to a CSV file."""
    with open(output_path, "w") as f:
        # Header
        f.write("filename," + ",".join(CLASSES) + "\n")

        # Data rows
        for filename in sorted(results.keys()):
            raw_preds = results[filename]["raw_predictions"]
            row = [filename] + [f"{p:.6f}" for p in raw_preds]
            f.write(",".join(row) + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="Run inference on spectrogram images using PNW-Cnet-5 ONNX model"
    )
    parser.add_argument(
        "input_dir",
        type=str,
        help="Directory containing spectrogram PNG images",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=DEFAULT_MODEL_PATH,
        help=f"Path to ONNX model (default: {DEFAULT_MODEL_PATH})",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Confidence threshold for detections (default: 0.5)",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="Number of top predictions to show (default: 5)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="Batch size for inference (default: 32)",
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Output file for results (CSV format)",
    )

    args = parser.parse_args()

    # Validate input directory
    input_path = Path(args.input_dir)
    if not input_path.is_dir():
        print(f"Error: {args.input_dir} is not a valid directory")
        return 1

    # Check for PNG files
    png_files = list(input_path.glob("*.png"))
    if not png_files:
        print(f"Error: No PNG files found in {args.input_dir}")
        return 1

    print(f"Found {len(png_files)} spectrogram images")

    # Validate model path
    if not Path(args.model).exists():
        print(f"Error: Model file not found: {args.model}")
        return 1

    # Run inference
    results = run_inference(
        args.input_dir,
        args.model,
        args.threshold,
        args.top_k,
        args.batch_size,
    )

    # Print results
    print_results(results, args.threshold, args.top_k)

    # Save to CSV if requested
    if args.output:
        save_results_csv(results, args.output)
        print(f"\nResults saved to: {args.output}")

    return 0


if __name__ == "__main__":
    exit(main())
