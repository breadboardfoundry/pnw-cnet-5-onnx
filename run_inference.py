"""
Run inference on spectrogram images using the PNW-Cnet-5 H5 model.

This script loads spectrogram images generated by wav_to_spectrogram.py
and runs them through the trained classifier to predict species.
"""

import argparse
import os
from pathlib import Path

import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten


# Class labels for the 135 bioacoustic categories (from Train_Model.py)
CLASSES = [
    "ACCO1", "ACGE1", "ACGE2", "ACST1", "AEAC1", "AEAC2", "Airplane", "ANCA1",
    "ASOT1", "BOUM1", "BRCA1", "BRMA1", "BRMA2", "BUJA1", "BUJA2", "Bullfrog",
    "BUVI1", "BUVI2", "CACA1", "CAGU1", "CAGU2", "CAGU3", "CALA1", "CALU1",
    "CAPU1", "CAUS1", "CAUS2", "CCOO1", "CCOO2", "CECA1", "Chainsaw", "CHFA1",
    "Chicken", "CHMI1", "CHMI2", "COAU1", "COAU2", "COBR1", "COCO1", "COSO1",
    "Cow", "Creek", "Cricket", "CYST1", "CYST2", "DEFU1", "DEFU2", "Dog",
    "DRPU1", "Drum", "EMDI1", "EMOB1", "FACO1", "FASP1", "Fly", "Frog",
    "GADE1", "GLGN1", "Growler", "Gunshot", "HALE1", "HAPU1", "HEVE1",
    "Highway", "Horn", "Human", "HYPI1", "IXNA1", "IXNA2", "JUHY1", "LEAL1",
    "LECE1", "LEVI1", "LEVI2", "LOCU1", "MEFO1", "MEGA1", "MEKE1", "MEKE2",
    "MEKE3", "MYTO1", "NUCO1", "OCPR1", "ODOC1", "ORPI1", "ORPI2", "PAFA1",
    "PAFA2", "PAHA1", "PECA1", "PHME1", "PHNU1", "PILU1", "PILU2", "PIMA1",
    "PIMA2", "POEC1", "POEC2", "PSFL1", "Rain", "Raptor", "SICU1", "SITT1",
    "SITT2", "SPHY1", "SPHY2", "SPPA1", "SPPI1", "SPTH1", "STDE1", "STNE1",
    "STNE2", "STOC_4Note", "STOC_Series", "Strix_Bark", "Strix_Whistle",
    "STVA_8Note", "STVA_Insp", "STVA_Series", "Survey_Tone", "TADO1", "TADO2",
    "TAMI1", "Thunder", "TRAE1", "Train", "Tree", "TUMI1", "TUMI2", "URAM1",
    "VIHU1", "Wildcat", "Yarder", "ZEMA1", "ZOLE1",
]

DEFAULT_MODEL_PATH = "model/Final_Model.h5"


def build_model(nclasses: int) -> Sequential:
    """
    Build the PNW-Cnet-5 CNN architecture.

    Architecture matches Train_Model.py with:
    - 6 Conv2D blocks with 32->32->64->64->128->128 filters
    - Dropout of 0.30 throughout
    - Dense layer with 512 units

    Args:
        nclasses: Number of output classes

    Returns:
        Keras Sequential model
    """
    dropout_prop = 0.30
    n_fc_nodes = 512

    model = Sequential()

    # Block 1
    model.add(
        Conv2D(
            32, (5, 5),
            input_shape=(257, 1000, 1),
            data_format="channels_last",
            activation="relu",
            padding="same",
        )
    )
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(dropout_prop))

    # Block 2
    model.add(Conv2D(32, (5, 5), activation="relu", padding="same"))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(dropout_prop))

    # Block 3
    model.add(Conv2D(64, (5, 5), activation="relu", padding="same"))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(dropout_prop))

    # Block 4
    model.add(Conv2D(64, (5, 5), activation="relu", padding="same"))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(dropout_prop))

    # Block 5
    model.add(Conv2D(128, (5, 5), activation="relu", padding="same"))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(dropout_prop))

    # Block 6
    model.add(Conv2D(128, (5, 5), activation="relu", padding="same"))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(dropout_prop))

    model.add(Flatten())

    # Dense layers
    model.add(Dense(n_fc_nodes, activation="relu"))
    model.add(Dropout(dropout_prop))

    model.add(Dense(nclasses, activation="sigmoid"))

    return model


def load_model(model_path: str) -> Sequential:
    """
    Build model architecture and load weights.

    Args:
        model_path: Path to the H5 weights file

    Returns:
        Model with loaded weights
    """
    model = build_model(len(CLASSES))
    model.load_weights(model_path)
    return model


def run_inference(
    input_dir: str,
    model_path: str,
    threshold: float = 0.5,
    top_k: int = 5,
) -> dict:
    """
    Run inference on all spectrogram images in a directory.

    Args:
        input_dir: Directory containing spectrogram PNG files
        model_path: Path to the H5 model weights
        threshold: Confidence threshold for predictions
        top_k: Number of top predictions to show per image

    Returns:
        Dictionary mapping filenames to predictions
    """
    # Load model
    print(f"Loading model from: {model_path}")
    model = load_model(model_path)

    # Load images using Keras preprocessing
    print(f"Loading spectrograms from: {input_dir}")
    dataset = tf.keras.preprocessing.image_dataset_from_directory(
        directory=input_dir,
        labels=None,
        class_names=None,
        color_mode="grayscale",
        batch_size=32,
        image_size=(257, 1000),
        shuffle=False,
    )

    # Get filenames in order
    filenames = dataset.file_paths

    # Normalize pixel values to [0, 1]
    normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)
    normalized_ds = dataset.map(lambda x: normalization_layer(x))

    # Run inference
    print("Running inference...")
    predictions = model.predict(normalized_ds, verbose=1)

    # Process results
    results = {}
    for i, filename in enumerate(filenames):
        pred = predictions[i]
        basename = os.path.basename(filename)

        # Get top-k predictions
        top_indices = np.argsort(pred)[::-1][:top_k]
        top_preds = [
            (CLASSES[idx], float(pred[idx]))
            for idx in top_indices
        ]

        # Get predictions above threshold
        above_threshold = [
            (CLASSES[idx], float(pred[idx]))
            for idx in range(len(pred))
            if pred[idx] >= threshold
        ]
        above_threshold.sort(key=lambda x: x[1], reverse=True)

        results[basename] = {
            "top_k": top_preds,
            "above_threshold": above_threshold,
            "raw_predictions": pred,
        }

    return results


def print_results(results: dict, threshold: float, top_k: int) -> None:
    """Print inference results in a readable format."""
    print("\n" + "=" * 60)
    print("INFERENCE RESULTS")
    print("=" * 60)

    for filename, data in sorted(results.items()):
        print(f"\n{filename}")
        print("-" * 40)

        if data["above_threshold"]:
            print(f"  Detections (threshold >= {threshold}):")
            for label, conf in data["above_threshold"]:
                print(f"    {label}: {conf:.4f}")
        else:
            print(f"  No detections above threshold {threshold}")

        print(f"  Top {top_k} predictions:")
        for label, conf in data["top_k"]:
            print(f"    {label}: {conf:.4f}")


def main():
    parser = argparse.ArgumentParser(
        description="Run inference on spectrogram images using PNW-Cnet-5"
    )
    parser.add_argument(
        "input_dir",
        type=str,
        help="Directory containing spectrogram PNG images",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=DEFAULT_MODEL_PATH,
        help=f"Path to H5 model weights (default: {DEFAULT_MODEL_PATH})",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Confidence threshold for detections (default: 0.5)",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="Number of top predictions to show (default: 5)",
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Output file for results (CSV format)",
    )

    args = parser.parse_args()

    # Validate input directory
    input_path = Path(args.input_dir)
    if not input_path.is_dir():
        print(f"Error: {args.input_dir} is not a valid directory")
        return 1

    # Check for PNG files
    png_files = list(input_path.glob("*.png"))
    if not png_files:
        print(f"Error: No PNG files found in {args.input_dir}")
        return 1

    print(f"Found {len(png_files)} spectrogram images")

    # Validate model path
    if not Path(args.model).exists():
        print(f"Error: Model file not found: {args.model}")
        return 1

    # Run inference
    results = run_inference(
        args.input_dir,
        args.model,
        args.threshold,
        args.top_k,
    )

    # Print results
    print_results(results, args.threshold, args.top_k)

    # Save to CSV if requested
    if args.output:
        save_results_csv(results, args.output)
        print(f"\nResults saved to: {args.output}")

    return 0


def save_results_csv(results: dict, output_path: str) -> None:
    """Save results to a CSV file."""
    with open(output_path, "w") as f:
        # Header
        f.write("filename," + ",".join(CLASSES) + "\n")

        # Data rows
        for filename in sorted(results.keys()):
            raw_preds = results[filename]["raw_predictions"]
            row = [filename] + [f"{p:.6f}" for p in raw_preds]
            f.write(",".join(row) + "\n")


if __name__ == "__main__":
    exit(main())
